#!/usr/bin/env python3
"""
Letta Sync - Syncs Aletheia facts and agent memory into Letta's archival memory

This script:
1. Creates an "aletheia-facts" archive if it doesn't exist
2. Reads facts.jsonl and syncs new/updated facts into Letta
3. Tracks what's been synced (incremental updates)
4. Syncs key curated memory from agent MEMORY.md files  
5. Is idempotent - running twice doesn't duplicate

Author: Syn (subagent for letta-wiring task)
Created: 2026-02-13
"""

import json
import logging
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any
import requests

# Configuration
LETTA_BASE_URL = "http://localhost:8283"
FACTS_FILE = Path("/mnt/ssd/aletheia/shared/memory/facts.jsonl")
STATE_FILE = Path("/mnt/ssd/aletheia/shared/memory/.letta-sync-state.json")
AGENT_MEMORY_DIR = Path("/mnt/ssd/aletheia/nous")
SHARED_DIR = Path("/mnt/ssd/aletheia/shared")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(SHARED_DIR / "logs" / "letta-sync.log")
    ]
)
logger = logging.getLogger(__name__)

class LettaAPIError(Exception):
    """Custom exception for Letta API errors"""
    pass

class LettaSync:
    def __init__(self):
        self.session = requests.Session()
        self.archive_id = None
        self.state = self.load_state()
        
    def load_state(self) -> Dict[str, Any]:
        """Load sync state from file"""
        if STATE_FILE.exists():
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
        return {
            "last_synced_fact_line": 0,
            "last_synced_timestamp": None,
            "synced_memory_files": {},
            "archive_id": None
        }
    
    def save_state(self):
        """Save sync state to file"""
        STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(STATE_FILE, 'w') as f:
            json.dump(self.state, f, indent=2)
    
    def api_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:
        """Make API request to Letta"""
        url = f"{LETTA_BASE_URL}{endpoint}"
        try:
            response = self.session.request(method, url, **kwargs)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise LettaAPIError(f"API request failed: {e}")
    
    def ensure_archive_exists(self) -> str:
        """Ensure the aletheia-facts archive exists, create if not"""
        # Check if we have a cached archive_id
        if self.state.get("archive_id"):
            try:
                # Verify it still exists
                self.api_request("GET", f"/v1/archives/{self.state['archive_id']}")
                return self.state["archive_id"]
            except LettaAPIError:
                logger.warning("Cached archive_id is invalid, will create new archive")
                self.state["archive_id"] = None
        
        # List existing archives
        archives = self.api_request("GET", "/v1/archives/")
        
        # Look for existing aletheia-facts archive
        for archive in archives:
            if archive["name"] == "aletheia-facts":
                archive_id = archive["id"]
                self.state["archive_id"] = archive_id
                self.save_state()
                logger.info(f"Found existing archive: {archive_id}")
                return archive_id
        
        # Create new archive
        archive_data = {
            "name": "aletheia-facts",
            "description": "Aletheia knowledge base - facts and agent memories",
            "embedding": "ollama-local/nomic-embed-text:latest"
        }
        
        archive = self.api_request("POST", "/v1/archives/", json=archive_data)
        archive_id = archive["id"]
        self.state["archive_id"] = archive_id
        self.save_state()
        logger.info(f"Created new archive: {archive_id}")
        return archive_id
    
    def sync_facts(self):
        """Sync new facts from facts.jsonl"""
        if not FACTS_FILE.exists():
            logger.warning(f"Facts file not found: {FACTS_FILE}")
            return
        
        archive_id = self.ensure_archive_exists()
        lines_processed = 0
        new_facts = 0
        
        # Process in smaller batches and save state frequently
        batch_size = 10
        
        with open(FACTS_FILE, 'r') as f:
            for line_num, line in enumerate(f, 1):
                # Skip already processed lines
                if line_num <= self.state["last_synced_fact_line"]:
                    continue
                
                try:
                    fact = json.loads(line.strip())
                    
                    # Convert fact to passage text
                    passage_text = self.fact_to_passage_text(fact)
                    
                    # Create passage in archive
                    passage_data = {
                        "text": passage_text,
                        "metadata": {
                            "fact_id": fact.get("id"),
                            "category": fact.get("category"),
                            "confidence": fact.get("confidence"),
                            "valid_from": fact.get("valid_from"),
                            "source": "facts.jsonl",
                            "sync_timestamp": datetime.now(timezone.utc).isoformat()
                        },
                        "tags": [
                            fact.get("category", "unknown"),
                            f"confidence-{int((fact.get('confidence', 0.5) * 10) // 1)}",
                            "fact"
                        ]
                    }
                    
                    self.api_request("POST", f"/v1/archives/{archive_id}/passages", json=passage_data)
                    new_facts += 1
                    lines_processed += 1
                    
                    # Save progress every batch_size items
                    if lines_processed % batch_size == 0:
                        self.state["last_synced_fact_line"] = self.state["last_synced_fact_line"] + lines_processed
                        self.save_state()
                        logger.info(f"Processed {lines_processed} facts so far...")
                        lines_processed = 0  # Reset for next batch
                    
                except json.JSONDecodeError:
                    logger.warning(f"Invalid JSON at line {line_num}: {line[:50]}...")
                    lines_processed += 1  # Still count as processed
                    continue
                except LettaAPIError as e:
                    logger.error(f"Failed to sync fact at line {line_num}: {e}")
                    lines_processed += 1  # Still count as processed to avoid retry loops
                    continue
                except Exception as e:
                    logger.error(f"Unexpected error at line {line_num}: {e}")
                    lines_processed += 1
                    continue
        
        # Update state with remaining items
        if lines_processed > 0:
            self.state["last_synced_fact_line"] = self.state["last_synced_fact_line"] + lines_processed
            self.state["last_synced_timestamp"] = datetime.now(timezone.utc).isoformat()
            self.save_state()
            
        logger.info(f"Facts sync complete: {new_facts} new facts added")
    
    def fact_to_passage_text(self, fact: Dict[str, Any]) -> str:
        """Convert a fact JSON object to readable passage text"""
        subject = fact.get("subject", "unknown")
        predicate = fact.get("predicate", "")
        obj = fact.get("object", "")
        confidence = fact.get("confidence", 0.5)
        category = fact.get("category", "unknown")
        
        # Format the fact as natural language
        text = f"FACT: {subject} {predicate} {obj}"
        
        # Add metadata as context
        metadata_parts = []
        if confidence:
            metadata_parts.append(f"confidence: {confidence:.2f}")
        if category:
            metadata_parts.append(f"category: {category}")
        if fact.get("valid_from"):
            metadata_parts.append(f"valid from: {fact['valid_from']}")
        if fact.get("reason"):
            metadata_parts.append(f"reason: {fact['reason']}")
        if fact.get("source_file"):
            metadata_parts.append(f"source: {fact['source_file']}")
            
        if metadata_parts:
            text += f" ({', '.join(metadata_parts)})"
        
        return text
    
    def sync_memory_files(self):
        """Sync MEMORY.md files from agents"""
        archive_id = self.ensure_archive_exists()
        updated_memories = 0
        
        # Find all MEMORY.md files
        memory_files = list(AGENT_MEMORY_DIR.glob("*/MEMORY.md"))
        
        for memory_file in memory_files:
            agent_name = memory_file.parent.name
            
            # Check if file has been modified since last sync
            file_mtime = memory_file.stat().st_mtime
            last_synced_mtime = self.state["synced_memory_files"].get(str(memory_file), 0)
            
            if file_mtime <= last_synced_mtime:
                continue  # File hasn't changed
            
            try:
                content = memory_file.read_text()
                
                # Skip empty files
                if not content.strip():
                    continue
                
                # Chunk large files by markdown sections (Letta has passage size limits)
                chunks = self.chunk_markdown(content, max_chars=4000)
                
                for i, chunk in enumerate(chunks):
                    passage_text = f"AGENT MEMORY - {agent_name.upper()}"
                    if len(chunks) > 1:
                        passage_text += f" (part {i+1}/{len(chunks)})"
                    passage_text += f"\n\n{chunk}"
                    
                    passage_data = {
                        "text": passage_text,
                        "metadata": {
                            "agent_name": agent_name,
                            "source": "MEMORY.md",
                            "file_path": str(memory_file),
                            "chunk_index": i,
                            "chunk_total": len(chunks),
                            "sync_timestamp": datetime.now(timezone.utc).isoformat(),
                            "file_mtime": file_mtime
                        },
                        "tags": [
                            "agent-memory",
                            f"agent-{agent_name}",
                            "curated"
                        ]
                    }
                    
                    self.api_request("POST", f"/v1/archives/{archive_id}/passages", json=passage_data)
                
                updated_memories += 1
                
                # Update state
                self.state["synced_memory_files"][str(memory_file)] = file_mtime
                
            except Exception as e:
                logger.error(f"Failed to sync {memory_file}: {e}")
                continue
        
        if updated_memories > 0:
            self.save_state()
            
        logger.info(f"Memory sync complete: {updated_memories} agent memories updated")
    
    def chunk_markdown(self, text: str, max_chars: int = 4000) -> List[str]:
        """Split markdown by sections, keeping chunks under max_chars."""
        import re
        # Split on ## headers (keep the header with its content)
        sections = re.split(r'(?=^## )', text, flags=re.MULTILINE)
        sections = [s.strip() for s in sections if s.strip()]
        
        chunks = []
        current = ""
        for section in sections:
            if len(current) + len(section) + 2 > max_chars and current:
                chunks.append(current)
                current = section
            else:
                current = current + "\n\n" + section if current else section
        if current:
            chunks.append(current)
        
        # If any chunk is still too large, split by paragraphs
        final_chunks = []
        for chunk in chunks:
            if len(chunk) <= max_chars:
                final_chunks.append(chunk)
            else:
                paragraphs = chunk.split("\n\n")
                sub = ""
                for para in paragraphs:
                    if len(sub) + len(para) + 2 > max_chars and sub:
                        final_chunks.append(sub)
                        sub = para
                    else:
                        sub = sub + "\n\n" + para if sub else para
                if sub:
                    final_chunks.append(sub)
        
        return final_chunks if final_chunks else [text[:max_chars]]

    def run(self):
        """Main sync operation"""
        logger.info("Starting Letta sync")
        
        try:
            # Sync facts from facts.jsonl
            self.sync_facts()
            
            # Sync agent MEMORY.md files
            self.sync_memory_files()
            
            logger.info("Letta sync completed successfully")
            
        except Exception as e:
            logger.error(f"Letta sync failed: {e}")
            sys.exit(1)

def main():
    """Entry point"""
    if len(sys.argv) > 1 and sys.argv[1] in ["-h", "--help"]:
        print(__doc__)
        sys.exit(0)
    
    # Ensure log directory exists
    log_dir = SHARED_DIR / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    sync = LettaSync()
    sync.run()

if __name__ == "__main__":
    main()