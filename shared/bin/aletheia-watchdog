#!/bin/bash
set -eo pipefail
set -a
source /mnt/ssd/aletheia/shared/config/aletheia.env
set +a 2>/dev/null || true

# aletheia-watchdog — Check critical subprocess health + auto-recovery
# Run via cron every 5 minutes

LOG="/tmp/aletheia-watchdog.log"
ALERT_FILE="/tmp/aletheia-watchdog-alerts.txt"
AGENT_ALERTS="/mnt/ssd/aletheia/nous/syn/ALERTS.md"
FAIL_COUNT_FILE="/tmp/aletheia-watchdog-fails"
STATUS_FILE="/mnt/ssd/aletheia/shared/status/services.json"
ALERTS_FILE="/mnt/ssd/aletheia/nous/syn/ALERTS.md"
MAX_FAILURES=3  # Consecutive failures before attempting checkpoint restore
MAX_MEM_MB=3500

log() { echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" >> "$LOG"; }

# Initialize status file if it doesn't exist
if [[ ! -f "$STATUS_FILE" ]]; then
    mkdir -p "$(dirname "$STATUS_FILE")"
    echo '{}' > "$STATUS_FILE"
fi

# Update service status in JSON file
update_service_status() {
    local service="$1"
    local status="$2"
    local error_msg="$3"
    local timestamp=$(date -Iseconds)
    
    # Use jq to update the JSON file atomically
    jq --arg service "$service" \
       --arg status "$status" \
       --arg timestamp "$timestamp" \
       --arg error "$error_msg" \
       '.[$service] = {
           "status": $status,
           "last_checked": $timestamp,
           "error_message": ($error // null)
       }' "$STATUS_FILE" > "${STATUS_FILE}.tmp" && mv "${STATUS_FILE}.tmp" "$STATUS_FILE"
}

# Check service health with timeout
check_service() {
    local service_name="$1"
    local check_command="$2"
    local timeout_seconds="${3:-10}"
    
    log "Checking $service_name..."
    
    if timeout "$timeout_seconds" bash -c "$check_command" >/dev/null 2>&1; then
        update_service_status "$service_name" "healthy" ""
        log "$service_name: OK"
        return 0
    else
        local error_msg=$(timeout "$timeout_seconds" bash -c "$check_command" 2>&1 | tail -1 || echo "Command failed or timed out")
        update_service_status "$service_name" "unhealthy" "$error_msg"
        log "$service_name: FAIL - $error_msg"
        return 1
    fi
}

get_fails() { cat "$FAIL_COUNT_FILE" 2>/dev/null || echo 0; }
set_fails() { echo "$1" > "$FAIL_COUNT_FILE"; }

# --- Service Health Checks ---

# Check Letta service
check_service "letta" "curl -sf http://localhost:8283/v1/health" 10 || true

# Check gcal CLI
check_service "gcal" "gcal today -c cody.kickertz@gmail.com" 15 || true

# Check NAS SSH connectivity
check_service "nas-ssh" "ssh -o ConnectTimeout=5 -o BatchMode=yes 192.168.0.120 echo ok" 10 || true

# --- Gateway Health ---
GATEWAY_PID=$(pgrep -f "aletheia-gateway\|aletheia gateway" 2>/dev/null | head -1 || true)

if [[ -z "$GATEWAY_PID" ]]; then
    FAILS=$(get_fails)
    FAILS=$((FAILS + 1))
    set_fails "$FAILS"
    log "CRITICAL: Gateway not running (failure $FAILS/$MAX_FAILURES)"

    if [[ "$FAILS" -ge "$MAX_FAILURES" ]]; then
        log "RECOVERY: $MAX_FAILURES consecutive failures — attempting checkpoint restore"

        # Try restore first
        if checkpoint verify >/dev/null 2>&1; then
            log "RECOVERY: System healthy after all — skipping restore"
        else
            checkpoint restore 2>&1 >> "$LOG" || true
        fi

        # Restart regardless
        if systemctl is-active --quiet aletheia; then
            sudo systemctl restart aletheia.service 2>&1 >> "$LOG"
            log "RECOVERY: Service restarted"
        else
            sudo systemctl start aletheia.service 2>&1 >> "$LOG"
            log "RECOVERY: Service started"
        fi

        set_fails 0
        echo "Gateway crashed $MAX_FAILURES times, auto-recovered $(date)" >> "$ALERT_FILE"
    else
        # Simple restart attempt
        sudo systemctl restart aletheia.service 2>&1 >> "$LOG" || true
        log "Restart attempt $FAILS"
    fi
else
    # Reset failure counter on success
    set_fails 0

    # Memory check
    MEM_MB=$(ps -o rss= -p "$GATEWAY_PID" 2>/dev/null | awk '{print int($1/1024)}' || true)
    if [[ -n "$MEM_MB" && "$MEM_MB" -gt "$MAX_MEM_MB" ]]; then
        log "WARNING: Memory at ${MEM_MB}MB (threshold: ${MAX_MEM_MB}MB)"
        echo "High memory: ${MEM_MB}MB at $(date)" >> "$ALERT_FILE"
    fi
fi

# --- Signal-CLI Health ---
if ! pgrep -f "signal-cli" >/dev/null 2>&1; then
    log "WARNING: signal-cli not running"
    # signal-cli is managed by the gateway, just log
fi

# --- Write ALERTS.md for agent heartbeat pickup ---
# Parse services.json for unhealthy services that were previously healthy
if [[ -f "$STATUS_FILE" ]]; then
    UNHEALTHY=$(python3 -c "
import json, sys
try:
    with open('$STATUS_FILE') as f:
        services = json.load(f)
    alerts = []
    for name, info in services.items():
        if info.get('status') == 'unhealthy':
            alerts.append(f'- **{name}**: {info.get(\"error_message\", \"unknown error\")[:100]}')
    if alerts:
        print('\n'.join(alerts))
except: pass
" 2>/dev/null || true)

    if [[ -n "$UNHEALTHY" ]]; then
        cat > "$AGENT_ALERTS" <<EOF
# ⚠️ Service Alerts
*Generated by aletheia-watchdog at $(date -Iseconds)*

$UNHEALTHY
EOF
    else
        # Clear alerts if everything is healthy
        rm -f "$AGENT_ALERTS" 2>/dev/null || true
    fi
fi

# --- Auto-checkpoint on healthy state (once per day) ---
TODAY=$(date +%Y%m%d)
LAST_CHECKPOINT="/tmp/aletheia-last-checkpoint"
LAST_CP_DATE=$(cat "$LAST_CHECKPOINT" 2>/dev/null || echo "none")

if [[ "$LAST_CP_DATE" != "$TODAY" ]] && [[ -n "$GATEWAY_PID" ]]; then
    if checkpoint verify >/dev/null 2>&1; then
        checkpoint save --label "daily-auto" >/dev/null 2>&1 || true
        echo "$TODAY" > "$LAST_CHECKPOINT"
        log "Daily auto-checkpoint saved"
    fi
fi
