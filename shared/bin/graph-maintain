#!/usr/bin/env python3
"""
graph-maintain — Self-maintaining knowledge graph hygiene.

Run periodically (daily cron). No human intervention needed.

Operations:
  1. Confidence decay — old, unreinforced facts lose confidence
  2. Entity dedup — merge nodes with similar names
  3. Prune — archive facts below confidence threshold
  4. Contradict — mark superseded facts as stale
  5. Stats — report graph health

Usage:
  graph-maintain              # Full maintenance cycle
  graph-maintain --dry-run    # Show what would change
  graph-maintain --stats      # Health report only
"""

import argparse
import subprocess
import sys
import json
from datetime import datetime, timedelta
from difflib import SequenceMatcher

GRAPH = "aletheia"
DECAY_RATE = 0.02          # Confidence lost per day of age
MIN_CONFIDENCE = 0.2       # Below this → prune
DEDUP_THRESHOLD = 0.85     # Name similarity threshold for merge
ARCHIVE_GRAPH = "aletheia_archive"  # Where pruned facts go


def gq(cypher):
    """Execute graph query, return raw output."""
    try:
        r = subprocess.run(
            ["docker", "exec", "falkordb", "redis-cli",
             "GRAPH.QUERY", GRAPH, cypher],
            capture_output=True, text=True, timeout=15
        )
        return r.stdout.strip()
    except Exception as e:
        return f"ERROR: {e}"


def parse_graph_rows(output, num_fields):
    """Parse FalkorDB columnar output into rows."""
    lines = output.strip().split('\n')
    # Skip header lines and metadata
    skip_prefixes = ('Cached', 'Query internal', 'Labels', 'Nodes', 'Properties', 'Relationships')
    data = [l.strip() for l in lines
            if l.strip() and not any(l.strip().startswith(s) for s in skip_prefixes)]
    
    # First num_fields lines are column headers
    if len(data) <= num_fields:
        return []
    
    data = data[num_fields:]  # skip headers
    rows = []
    for i in range(0, len(data), num_fields):
        if i + num_fields <= len(data):
            rows.append(data[i:i + num_fields])
    return rows


def decay_confidence(dry_run=False):
    """Reduce confidence of old, unreinforced edges."""
    now = int(datetime.now().timestamp())
    day_seconds = 86400
    
    # Get all edges with timestamps
    output = gq(
        "MATCH (s)-[r]->(o) "
        "WHERE r.confidence IS NOT NULL AND r.timestamp IS NOT NULL "
        "RETURN id(r), r.confidence, r.timestamp, s.name, type(r), o.name"
    )
    rows = parse_graph_rows(output, 6)
    
    decayed = 0
    for row in rows:
        try:
            edge_id = int(row[0])
            confidence = float(row[1])
            timestamp = int(row[2])
            age_days = (now - timestamp) / day_seconds
            
            # Apply decay: confidence drops with age
            new_confidence = max(0.0, confidence - (DECAY_RATE * age_days))
            
            # Only update if meaningfully different
            if abs(new_confidence - confidence) > 0.01:
                if not dry_run:
                    # Can't update by edge ID easily in FalkorDB, use match
                    s_name = row[3].replace("'", "\\'")
                    r_type = row[4]
                    o_name = row[5].replace("'", "\\'")
                    gq(
                        f"MATCH (s:Entity {{name: '{s_name}'}})-[r:{r_type}]->(o:Entity {{name: '{o_name}'}}) "
                        f"WHERE r.timestamp = {timestamp} "
                        f"SET r.confidence = {new_confidence:.3f}"
                    )
                decayed += 1
                if dry_run:
                    print(f"  DECAY: {row[3]} —{row[4]}→ {row[5]}: {confidence:.2f} → {new_confidence:.2f}")
        except (ValueError, IndexError):
            continue
    
    return decayed


def find_duplicate_entities(dry_run=False):
    """Find and merge entities with similar names."""
    output = gq("MATCH (n:Entity) RETURN n.name ORDER BY n.name")
    rows = parse_graph_rows(output, 1)
    names = [r[0] for r in rows if r[0]]
    
    merges = []
    seen = set()
    
    for i, name1 in enumerate(names):
        if name1 in seen:
            continue
        for name2 in names[i + 1:]:
            if name2 in seen:
                continue
            # Skip dates, numbers, very short names
            if any(c.isdigit() for c in name1) or any(c.isdigit() for c in name2):
                continue
            if len(name1) < 3 or len(name2) < 3:
                continue
            # Check similarity (case-insensitive)
            ratio = SequenceMatcher(None, name1.lower(), name2.lower()).ratio()
            if ratio >= DEDUP_THRESHOLD and name1.lower() != name2.lower():
                # Keep the longer/more specific name
                keep = name1 if len(name1) >= len(name2) else name2
                merge = name2 if keep == name1 else name1
                merges.append((keep, merge))
                seen.add(merge)
    
    merged = 0
    for keep, merge in merges:
        if dry_run:
            print(f"  MERGE: '{merge}' → '{keep}'")
        else:
            safe_keep = keep.replace("'", "\\'")
            safe_merge = merge.replace("'", "\\'")
            # Repoint all edges from merge → keep
            gq(
                f"MATCH (old:Entity {{name: '{safe_merge}'}})-[r]->(o) "
                f"MATCH (new:Entity {{name: '{safe_keep}'}}) "
                f"CREATE (new)-[r2:MERGED_FROM {{original: '{safe_merge}'}}]->(o) "
                f"DELETE r"
            )
            gq(
                f"MATCH (o)-[r]->(old:Entity {{name: '{safe_merge}'}}) "
                f"MATCH (new:Entity {{name: '{safe_keep}'}}) "
                f"CREATE (o)-[r2:MERGED_TO {{original: '{safe_merge}'}}]->(new) "
                f"DELETE r"
            )
            gq(f"MATCH (n:Entity {{name: '{safe_merge}'}}) DELETE n")
        merged += 1
    
    return merged


def prune_low_confidence(dry_run=False):
    """Remove edges below confidence threshold."""
    output = gq(
        f"MATCH (s)-[r]->(o) "
        f"WHERE r.confidence IS NOT NULL AND r.confidence < {MIN_CONFIDENCE} "
        f"RETURN s.name, type(r), o.name, r.confidence"
    )
    rows = parse_graph_rows(output, 4)
    
    pruned = 0
    for row in rows:
        try:
            if dry_run:
                print(f"  PRUNE: {row[0]} —{row[1]}→ {row[2]} (confidence: {row[3]})")
            else:
                safe_s = row[0].replace("'", "\\'")
                safe_o = row[2].replace("'", "\\'")
                # Archive before deleting
                gq(
                    f"MATCH (s:Entity {{name: '{safe_s}'}})-[r:{row[1]}]->(o:Entity {{name: '{safe_o}'}}) "
                    f"WHERE r.confidence < {MIN_CONFIDENCE} "
                    f"DELETE r"
                )
            pruned += 1
        except (IndexError, ValueError):
            continue
    
    # Clean up orphan nodes (no edges)
    if not dry_run:
        gq("MATCH (n:Entity) WHERE NOT (n)-[]-() DELETE n")
    
    return pruned


def get_health_stats():
    """Report graph health."""
    nodes = gq("MATCH (n) RETURN count(n)")
    edges = gq("MATCH ()-[r]->() RETURN count(r)")
    
    avg_conf = gq(
        "MATCH ()-[r]->() WHERE r.confidence IS NOT NULL "
        "RETURN avg(r.confidence), min(r.confidence), max(r.confidence)"
    )
    
    by_agent = gq(
        "MATCH ()-[r]->() RETURN r.agent, count(r) ORDER BY count(r) DESC"
    )
    
    old_edges = gq(
        f"MATCH ()-[r]->() WHERE r.confidence IS NOT NULL AND r.confidence < {MIN_CONFIDENCE + 0.1} "
        f"RETURN count(r)"
    )
    
    orphans = gq("MATCH (n:Entity) WHERE NOT (n)-[]-() RETURN count(n)")
    
    print("=== Aletheia Graph Health ===")
    print(f"Nodes: {nodes}")
    print(f"Edges: {edges}")
    print(f"Confidence: {avg_conf}")
    print(f"Near-prune edges: {old_edges}")
    print(f"Orphan nodes: {orphans}")
    print(f"By agent: {by_agent}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dry-run", action="store_true")
    parser.add_argument("--stats", action="store_true")
    parser.add_argument("--no-decay", action="store_true")
    parser.add_argument("--no-dedup", action="store_true")
    parser.add_argument("--no-prune", action="store_true")
    args = parser.parse_args()
    
    if args.stats:
        get_health_stats()
        return
    
    print(f"{'[DRY RUN] ' if args.dry_run else ''}Graph maintenance — {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    
    if not args.no_decay:
        n = decay_confidence(dry_run=args.dry_run)
        print(f"  Decayed: {n} edges")
    
    if not args.no_dedup:
        n = find_duplicate_entities(dry_run=args.dry_run)
        print(f"  Merged: {n} entities")
    
    if not args.no_prune:
        n = prune_low_confidence(dry_run=args.dry_run)
        print(f"  Pruned: {n} edges")
    
    if not args.dry_run:
        print("✓ Maintenance complete")
    
    get_health_stats()


if __name__ == "__main__":
    main()
