#!/usr/bin/env python3
"""
wiki — Wikipedia lookup for Aletheia.

Quick access to Wikipedia articles for concept verification and context.
Not a primary source — use for orientation and finding primary refs.

Usage:
    wiki "Poincaré section"              # Get article summary
    wiki "binding problem" --full        # Full article as markdown
    wiki "dynamical systems" --refs      # Extract references/citations
    wiki --search "topological dynamics" # Search for articles
"""

import json
import sys
import urllib.request
import urllib.parse

WIKI_API = "https://en.wikipedia.org/api/rest_v1"
WIKI_ACTION = "https://en.wikipedia.org/w/api.php"

def get_summary(title):
    """Get article summary."""
    encoded = urllib.parse.quote(title.replace(" ", "_"))
    url = f"{WIKI_API}/page/summary/{encoded}"
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Aletheia/1.0"})
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read())
        return {
            "title": data.get("title"),
            "extract": data.get("extract"),
            "description": data.get("description"),
            "url": data.get("content_urls", {}).get("desktop", {}).get("page"),
        }
    except Exception as e:
        return {"error": str(e)}

def search_wiki(query, limit=5):
    """Search Wikipedia."""
    params = {
        "action": "query",
        "list": "search",
        "srsearch": query,
        "srlimit": limit,
        "format": "json",
    }
    url = f"{WIKI_ACTION}?{urllib.parse.urlencode(params)}"
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Aletheia/1.0"})
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read())
        return [{"title": r["title"], "snippet": r.get("snippet", "")} 
                for r in data.get("query", {}).get("search", [])]
    except Exception as e:
        return [{"error": str(e)}]

def get_references(title):
    """Extract external references from an article."""
    params = {
        "action": "query",
        "titles": title,
        "prop": "extlinks",
        "ellimit": "50",
        "format": "json",
    }
    url = f"{WIKI_ACTION}?{urllib.parse.urlencode(params)}"
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Aletheia/1.0"})
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read())
        pages = data.get("query", {}).get("pages", {})
        for pid, page in pages.items():
            links = page.get("extlinks", [])
            return [l.get("*", l.get("url", "?")) for l in links]
    except Exception as e:
        return [str(e)]

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Wikipedia lookup")
    parser.add_argument("query", nargs="*")
    parser.add_argument("--search", help="Search mode")
    parser.add_argument("--refs", action="store_true", help="Get references")
    parser.add_argument("--full", action="store_true", help="Full article")
    parser.add_argument("--json", action="store_true")
    
    args = parser.parse_args()
    
    if args.search:
        results = search_wiki(args.search)
        for r in results:
            print(f"• {r['title']}")
        return
    
    if not args.query:
        parser.print_help()
        return
    
    title = " ".join(args.query)
    
    if args.refs:
        refs = get_references(title)
        print(f"References from: {title}")
        for r in refs:
            print(f"  • {r}")
        return
    
    summary = get_summary(title)
    if "error" in summary:
        # Try search instead
        print(f"No exact match. Searching...")
        results = search_wiki(title)
        for r in results:
            print(f"• {r['title']}")
        return
    
    if args.json:
        print(json.dumps(summary, indent=2))
    else:
        print(f"# {summary['title']}")
        if summary.get('description'):
            print(f"*{summary['description']}*")
        print()
        print(summary.get('extract', 'No content'))
        print(f"\n→ {summary.get('url', '')}")
        print(f"\n⚠️  Wikipedia is S4 (grey literature). Use for orientation, cite primary sources.")

if __name__ == "__main__":
    main()
