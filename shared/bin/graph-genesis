#!/usr/bin/env python3
"""
graph-genesis — Transform the aletheia graph from archival to generative.

Phase 1 of L5: Clean the ontology, deduplicate, assign domains,
then run link prediction and serendipity scoring.

Usage:
    graph-genesis clean       # Deduplicate edges, normalize relation types
    graph-genesis classify    # Assign domain tags to all nodes
    graph-genesis predict     # Run link prediction, propose hypothetical edges
    graph-genesis serendipity # Surface unexpected cross-domain connections
    graph-genesis report      # Full status report
    graph-genesis all         # Run everything in sequence
"""

import os
import sys
import json
from collections import Counter, defaultdict
from itertools import combinations
from datetime import datetime

ALETHEIA_ROOT = os.environ.get("ALETHEIA_ROOT", "/mnt/ssd/aletheia")

try:
    from falkordb import FalkorDB
except ImportError:
    print("ERROR: falkordb not installed. Run: pip install falkordb", file=sys.stderr)
    sys.exit(1)


def get_graph():
    db = FalkorDB(host="localhost", port=6379)
    return db.select_graph("aletheia")


# ─── Ontology: canonical relation types ───────────────────────────────────────

RELATION_ONTOLOGY = {
    # Core structural
    "IS_A": "IS_A",
    "IS": "IS_A",
    "TYPE": "IS_A",
    
    # Composition / ownership
    "HAS": "HAS",
    "OWNS": "OWNS",
    "CONTAINS": "HAS",
    "INCLUDES": "HAS",
    
    # Agent/role
    "ROLE": "HAS_ROLE",
    "MANAGES": "MANAGES",
    "MANAGED_BY": "MANAGED_BY",
    "CREATED": "CREATED",
    
    # Knowledge
    "KNOWS": "KNOWS",
    "LEARNED": "LEARNED",
    "LESSON": "LEARNED",
    "LESSON_LEARNED": "LEARNED",
    
    # Decision/preference
    "DECIDED": "DECIDED",
    "PREFERS": "PREFERS",
    "PREFERENCE": "PREFERS",
    
    # Relationships
    "FAMILY_MEMBER": "RELATED_TO",
    "RELATION_TO_CODY": "RELATED_TO",
    
    # Domain
    "DOMAIN": "IN_DOMAIN",
    
    # Causation
    "CAUSED_BY": "CAUSED_BY",
    "ROOT_CAUSE": "CAUSED_BY",
    "FIX": "FIXES",
    "FIX_APPLIED": "FIXES",
    
    # Materials/supplies
    "SUPPLIES": "SUPPLIES",
    "MATERIALS": "MADE_FROM",
    "MATERIALS_COST": "COSTS",
    "PRICE": "COSTS",
    "RETAIL_PRICE": "COSTS",
    
    # Construction/making
    "CONSTRUCTION": "CONSTRUCTED_AS",
    "CONSTRUCTION_V2": "CONSTRUCTED_AS",
    
    # Status
    "STATUS": "HAS_STATUS",
    "COMPLETED": "HAS_STATUS",
    "ACTIVE": "HAS_STATUS",
    
    # Architecture/infrastructure
    "ARCHITECTURE": "HAS_ARCHITECTURE",
    "CONFIGURED_FOR": "CONFIGURED_FOR",
    "USES": "USES",
    "TOOLS": "USES",
    "SOFTWARE": "USES",
    "MODEL": "USES_MODEL",
    
    # Temporal
    "BEFORE": "BEFORE",
    
    # Correction
    "CORRECTED": "CORRECTED",
    "REPLACES_CONCEPT": "REPLACES",
    "CHANGED_TO": "REPLACES",
    
    # Signal/comms
    "SIGNAL_GROUP": "HAS_SIGNAL_GROUP",
    "SIGNAL_UUID": "HAS_SIGNAL_UUID",
    
    # Incidents
    "INCIDENT": "HAD_INCIDENT",
    "EXPERIENCED_INCIDENT": "HAD_INCIDENT",
}

# Domain classification based on node name patterns
DOMAIN_PATTERNS = {
    "craft": [
        "ardent", "leather", "belt", "buckle", "stitch", "dye", "aima",
        "thanatochromia", "aporia", "keeper", "wax", "hermann", "abbey",
        "fil au chinois", "wickett", "springfield", "edge", "skive",
        "saddle", "creaser", "slicker", "burnish", "punch", "iron",
    ],
    "work": [
        "employer", "roi", "pepm", "redshift", "gnomon", "taxonomy",
        "dashboard", "icd", "claims", "data_landscape", "chiron",
        "client", "prospect", "employer", "engagement",
    ],
    "school": [
        "mba", "capstone", "tbc", "eiron", "acf", "ercot",
        "mba_program", "utexas", "professor_1", "professor_2", "macroeconomics",
        "workstream", "derek", "aaron", "evan", "marshall", "ramiro",
    ],
    "home": [
        "partner", "child", "pet_1", "pet_2", "pet_3", "syl",
        "home_city", "family", "house", "baby",
    ],
    "infrastructure": [
        "openclaw", "aletheia", "falkordb", "syncthing", "signal",
        "docker", "nas", "laptop", "server-host", "tailscale",
        "cron", "systemd", "nginx", "caddy", "mullvad",
    ],
    "cognition": [
        "nous", "metaxy", "dianoia", "distill", "compaction",
        "attention", "continuity", "character", "prosoche",
        "emergence", "topology", "gnosis", "noesis",
    ],
    "vehicle": [
        "truck", "cummins", "ram", "diesel", "vin", "mileage",
        "axle", "transmission", "engine", "akron",
    ],
    "identity": [
        "the operator", "audhd", "cognitive", "adhd", "autism", "iq",
        "masking", "translation", "activation", "resonance",
        "repulsion", "monotropism", "hayakawa",
    ],
}


def classify_domain(name: str) -> str:
    """Classify a node into a domain based on name patterns."""
    if not name:
        return "unknown"
    name_lower = name.lower()
    scores = {}
    for domain, patterns in DOMAIN_PATTERNS.items():
        score = sum(1 for p in patterns if p in name_lower)
        if score > 0:
            scores[domain] = score
    if scores:
        return max(scores, key=scores.get)
    return "general"


def cmd_clean():
    """Deduplicate edges, normalize relation types."""
    g = get_graph()
    
    print("=== Phase 1a: Graph Cleanup ===\n")
    
    # Count before
    before_edges = g.query("MATCH ()-[r]->() RETURN count(r)").result_set[0][0]
    before_types = g.query("MATCH ()-[r]->() RETURN count(DISTINCT type(r))").result_set[0][0]
    print(f"Before: {before_edges} edges, {before_types} relation types")
    
    # Find and remove duplicate edges
    dupes = g.query("""
        MATCH (a)-[r]->(b) 
        WITH a, type(r) as rel, b, collect(r) as edges 
        WHERE size(edges) > 1 
        RETURN a.name, rel, b.name, size(edges)
    """)
    
    dupe_count = 0
    for row in dupes.result_set:
        src, rel, tgt, count = row
        # Keep one, delete the rest
        to_delete = count - 1
        for _ in range(to_delete):
            try:
                g.query(f"""
                    MATCH (a {{name: $src}})-[r:{rel}]->(b {{name: $tgt}})
                    WITH r LIMIT 1
                    DELETE r
                """, {"src": src, "tgt": tgt})
                dupe_count += 1
            except Exception:
                pass
    
    print(f"Removed {dupe_count} duplicate edges")
    
    # After
    after_edges = g.query("MATCH ()-[r]->() RETURN count(r)").result_set[0][0]
    after_types = g.query("MATCH ()-[r]->() RETURN count(DISTINCT type(r))").result_set[0][0]
    print(f"After: {after_edges} edges, {after_types} relation types")
    print(f"Reduction: {before_edges - after_edges} edges removed")


def cmd_classify():
    """Assign domain tags to all nodes."""
    g = get_graph()
    
    print("=== Phase 1b: Domain Classification ===\n")
    
    result = g.query("MATCH (n) RETURN n.name")
    domain_counts = Counter()
    classified = 0
    
    for row in result.result_set:
        name = row[0]
        if not name:
            continue
        domain = classify_domain(name)
        domain_counts[domain] += 1
        try:
            g.query("MATCH (n {name: $name}) SET n.domain = $domain",
                     {"name": name, "domain": domain})
            classified += 1
        except Exception as e:
            print(f"  WARN: Failed to classify '{name}': {e}")
    
    print(f"Classified {classified} nodes:")
    for domain, count in domain_counts.most_common():
        print(f"  {domain}: {count}")


def cmd_predict():
    """Run link prediction — propose hypothetical edges based on structural patterns."""
    g = get_graph()
    
    print("=== Phase 1c: Link Prediction ===\n")
    
    # Strategy 1: Nodes that share many neighbors probably should be connected
    # (Common neighbor-based link prediction)
    candidates = g.query("""
        MATCH (a)-[]->(c)<-[]-(b)
        WHERE a <> b AND NOT (a)-[]-(b)
        WITH a.name as src, b.name as tgt, count(DISTINCT c) as common_neighbors, 
             collect(DISTINCT c.name) as shared
        WHERE common_neighbors >= 2
        RETURN src, tgt, common_neighbors, shared
        ORDER BY common_neighbors DESC
        LIMIT 30
    """)
    
    predictions = []
    print(f"Found {len(candidates.result_set)} candidate connections:\n")
    
    for row in candidates.result_set:
        src, tgt, cn, shared = row
        # Compute Jaccard-like score
        # Get total neighbors of each
        src_n = g.query("MATCH (n {name: $name})-[]-(m) RETURN count(DISTINCT m)", 
                        {"name": src}).result_set[0][0]
        tgt_n = g.query("MATCH (n {name: $name})-[]-(m) RETURN count(DISTINCT m)",
                        {"name": tgt}).result_set[0][0]
        
        if src_n + tgt_n - cn == 0:
            continue
            
        jaccard = cn / (src_n + tgt_n - cn)
        confidence = min(0.7, jaccard * 2)  # Cap at 0.7 for hypotheticals
        
        predictions.append({
            "source": src,
            "target": tgt,
            "common_neighbors": cn,
            "shared_via": shared[:5],
            "jaccard": round(jaccard, 3),
            "confidence": round(confidence, 3),
        })
        
        print(f"  {src} <-> {tgt}")
        print(f"    common neighbors: {cn} via {shared[:3]}")
        print(f"    jaccard: {jaccard:.3f}, confidence: {confidence:.3f}")
        print()
    
    # Strategy 2: Transitive closure — if A->B and B->C with same rel type, suggest A->C
    transitive = g.query("""
        MATCH (a)-[r1]->(b)-[r2]->(c)
        WHERE type(r1) = type(r2) AND a <> c AND NOT (a)-[]-(c)
        WITH a.name as src, type(r1) as rel, c.name as tgt, b.name as via, count(*) as paths
        WHERE paths >= 1
        RETURN DISTINCT src, rel, tgt, via, paths
        ORDER BY paths DESC
        LIMIT 20
    """)
    
    print(f"\nTransitive candidates ({len(transitive.result_set)}):\n")
    for row in transitive.result_set:
        src, rel, tgt, via, paths = row
        predictions.append({
            "source": src,
            "target": tgt,
            "relation": rel,
            "via": via,
            "type": "transitive",
            "confidence": 0.5,
        })
        print(f"  {src} --[{rel}]--> {tgt} (via {via})")
    
    # Write predictions to file
    outpath = os.path.join(ALETHEIA_ROOT, "shared", "memory", "graph-predictions.jsonl")
    with open(outpath, "w") as f:
        for p in predictions:
            p["timestamp"] = datetime.now().isoformat()
            p["status"] = "hypothetical"
            f.write(json.dumps(p) + "\n")
    
    print(f"\nWrote {len(predictions)} predictions to {outpath}")


def cmd_serendipity():
    """Surface unexpected cross-domain connections."""
    g = get_graph()
    
    print("=== Phase 1d: Serendipity Engine ===\n")
    
    # First ensure domains are set
    result = g.query("MATCH (n) WHERE n.domain IS NULL RETURN count(n)")
    unclassified = result.result_set[0][0]
    if unclassified > 50:
        print(f"  {unclassified} unclassified nodes — running classify first...")
        cmd_classify()
        print()
    
    # Find cross-domain connections (existing)
    cross = g.query("""
        MATCH (a)-[r]->(b)
        WHERE a.domain IS NOT NULL AND b.domain IS NOT NULL AND a.domain <> b.domain
        RETURN a.name, a.domain, type(r), b.name, b.domain, r.confidence
        ORDER BY r.confidence DESC
    """)
    
    print(f"Existing cross-domain connections: {len(cross.result_set)}\n")
    
    domain_bridges = defaultdict(list)
    for row in cross.result_set:
        src, src_dom, rel, tgt, tgt_dom, conf = row
        bridge = tuple(sorted([src_dom, tgt_dom]))
        domain_bridges[bridge].append({
            "from": src, "rel": rel, "to": tgt, "conf": conf
        })
    
    for bridge, connections in sorted(domain_bridges.items(), key=lambda x: len(x[1])):
        print(f"  {bridge[0]} <-> {bridge[1]}: {len(connections)} connections")
        for c in connections[:3]:
            print(f"    {c['from']} --[{c['rel']}]--> {c['to']}")
    
    # Find nodes that COULD bridge domains (2-hop paths across domains)
    print("\n--- Potential Cross-Domain Bridges ---\n")
    
    bridges = g.query("""
        MATCH (a)-[]->(mid)-[]->(b)
        WHERE a.domain IS NOT NULL AND b.domain IS NOT NULL 
          AND a.domain <> b.domain
          AND NOT (a)-[]-(b)
        WITH a.name as src, a.domain as src_dom, 
             mid.name as bridge_node,
             b.name as tgt, b.domain as tgt_dom,
             count(*) as path_count
        RETURN src, src_dom, bridge_node, tgt, tgt_dom, path_count
        ORDER BY path_count DESC
        LIMIT 20
    """)
    
    serendipity_finds = []
    for row in bridges.result_set:
        src, src_dom, bridge, tgt, tgt_dom, paths = row
        
        # Score: novelty (rare domain pair) × path_count
        pair = tuple(sorted([src_dom, tgt_dom]))
        existing_count = len(domain_bridges.get(pair, []))
        novelty = 1.0 / (1 + existing_count)  # Rarer pairs score higher
        score = novelty * paths
        
        find = {
            "source": src,
            "source_domain": src_dom,
            "target": tgt,
            "target_domain": tgt_dom,
            "bridge": bridge,
            "paths": paths,
            "novelty": round(novelty, 3),
            "score": round(score, 3),
        }
        serendipity_finds.append(find)
        
        if score > 0.3:
            print(f"  ★ {src} ({src_dom}) <-> {tgt} ({tgt_dom})")
            print(f"    via: {bridge} | novelty: {novelty:.2f} | score: {score:.2f}")
    
    # Write serendipity finds
    outpath = os.path.join(ALETHEIA_ROOT, "shared", "memory", "graph-serendipity.jsonl")
    with open(outpath, "w") as f:
        for s in serendipity_finds:
            s["timestamp"] = datetime.now().isoformat()
            f.write(json.dumps(s) + "\n")
    
    print(f"\nWrote {len(serendipity_finds)} serendipity finds to {outpath}")


def cmd_report():
    """Full graph status report."""
    g = get_graph()
    
    print("=== Aletheia Graph Report ===")
    print(f"    Generated: {datetime.now().strftime('%Y-%m-%d %H:%M CST')}\n")
    
    nodes = g.query("MATCH (n) RETURN count(n)").result_set[0][0]
    edges = g.query("MATCH ()-[r]->() RETURN count(r)").result_set[0][0]
    rel_types = g.query("MATCH ()-[r]->() RETURN count(DISTINCT type(r))").result_set[0][0]
    
    print(f"Nodes: {nodes}")
    print(f"Edges: {edges}")
    print(f"Relation types: {rel_types}")
    
    # Domain distribution
    domains = g.query("MATCH (n) WHERE n.domain IS NOT NULL RETURN n.domain, count(n) ORDER BY count(n) DESC")
    print(f"\nDomain distribution:")
    for row in domains.result_set:
        print(f"  {row[0]}: {row[1]}")
    
    # Cross-domain edge count
    cross = g.query("""
        MATCH (a)-[]->(b) 
        WHERE a.domain IS NOT NULL AND b.domain IS NOT NULL AND a.domain <> b.domain
        RETURN count(*)
    """).result_set[0][0]
    print(f"\nCross-domain edges: {cross}")
    
    # Hypothetical edges
    pred_path = os.path.join(ALETHEIA_ROOT, "shared", "memory", "graph-predictions.jsonl")
    if os.path.exists(pred_path):
        with open(pred_path) as f:
            preds = [json.loads(l) for l in f if l.strip()]
        print(f"Hypothetical edges: {len(preds)}")
    
    # Serendipity finds
    ser_path = os.path.join(ALETHEIA_ROOT, "shared", "memory", "graph-serendipity.jsonl")
    if os.path.exists(ser_path):
        with open(ser_path) as f:
            finds = [json.loads(l) for l in f if l.strip()]
        print(f"Serendipity finds: {len(finds)}")
        top = sorted(finds, key=lambda x: x.get("score", 0), reverse=True)[:5]
        if top:
            print("\nTop serendipity finds:")
            for t in top:
                print(f"  {t['source']} ({t['source_domain']}) <-> {t['target']} ({t['target_domain']}) via {t['bridge']}")


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        sys.exit(0)
    
    cmd = sys.argv[1]
    commands = {
        "clean": cmd_clean,
        "classify": cmd_classify,
        "predict": cmd_predict,
        "serendipity": cmd_serendipity,
        "report": cmd_report,
        "all": lambda: (cmd_clean(), print(), cmd_classify(), print(), 
                        cmd_predict(), print(), cmd_serendipity(), print(),
                        cmd_report()),
    }
    
    if cmd not in commands:
        print(f"Unknown command: {cmd}")
        print(f"Available: {', '.join(commands)}")
        sys.exit(1)
    
    commands[cmd]()


if __name__ == "__main__":
    main()
