#!/usr/bin/env python3
source /mnt/ssd/aletheia/shared/config/aletheia.env 2>/dev/null || true
"""
extract-insights - Extract decisions, preferences, and insights from conversation
Implements research-compaction.md findings for semantic context preservation

Usage:
    extract-insights --input <conversation.jsonl> --output <insights.json> [--workspace <path>]
    extract-insights --prompt  # Output Claude extraction prompt

Based on hierarchical summarization with importance weighting and decision tree extraction.
"""

import argparse
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional


# Importance weights for different content types (based on research-compaction.md)
IMPORTANCE_WEIGHTS = {
    'user_preference': 1.0,
    'decision_point': 1.0,
    'error_correction': 0.9,
    'novel_insight': 0.9,
    'cross_reference': 0.6,
    'procedural': 0.3,
    'acknowledgment': 0.1
}

# Decision markers for pattern detection
DECISION_MARKERS = [
    r'decided?\s+to',
    r'choice\s+is',
    r'going\s+with',
    r'let\'s\s+go\s+with',
    r'will\s+use',
    r'selected?',
    r'picked?',
    r'chose\w*',
    r'conclusion\s+is',
    r'final\s+answer',
    r'solution\s+is'
]

# Preference markers
PREFERENCE_MARKERS = [
    r'prefer\w*',
    r'like\w*\s+to',
    r'want\w*\s+to',
    r'should\s+be',
    r'better\s+if',
    r'would\s+rather',
    r'always\s+use',
    r'never\s+use',
    r'hate\s+when',
    r'love\s+when'
]

# Insight markers  
INSIGHT_MARKERS = [
    r'pattern\s+I\s+see',
    r'interesting\w*',
    r'realize\w*',
    r'noticed?\s+that',
    r'consistently',
    r'always\s+seems?',
    r'tendency\s+to',
    r'recurring\s+theme',
    r'emerges?\s+that'
]


class ConversationAnalyzer:
    """Analyzes conversation for decisions, preferences, and insights."""
    
    def __init__(self, workspace: Optional[str] = None):
        self.workspace = workspace or "${ALETHEIA_NOUS:-${ALETHEIA_ROOT:-/mnt/ssd/aletheia}/nous}/syn"
        self.turns = []
        self.decisions = []
        self.preferences = []
        self.insights = []
        self.reasoning_chains = []
        
    def load_conversation(self, file_path: str) -> None:
        """Load conversation from JSONL file."""
        try:
            with open(file_path, 'r') as f:
                for line_no, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        turn = json.loads(line)
                        turn['line_number'] = line_no
                        self.turns.append(turn)
                    except json.JSONDecodeError as e:
                        print(f"Warning: Invalid JSON on line {line_no}: {e}")
                        continue
        except FileNotFoundError:
            raise FileNotFoundError(f"Conversation file not found: {file_path}")
        except Exception as e:
            raise Exception(f"Error loading conversation: {e}")
    
    def extract_content(self, turn: Dict[str, Any]) -> str:
        """Extract text content from a conversation turn."""
        if 'content' in turn:
            if isinstance(turn['content'], str):
                return turn['content']
            elif isinstance(turn['content'], list):
                # Handle message with multiple content blocks
                text_parts = []
                for block in turn['content']:
                    if isinstance(block, dict) and block.get('type') == 'text':
                        text_parts.append(block.get('text', ''))
                    elif isinstance(block, str):
                        text_parts.append(block)
                return ' '.join(text_parts)
        elif 'message' in turn:
            return str(turn['message'])
        elif 'text' in turn:
            return str(turn['text'])
        
        # Fallback: convert entire turn to string and extract readable parts
        turn_str = str(turn)
        # Basic cleanup of JSON artifacts
        turn_str = re.sub(r'[{}\[\]"\':]', ' ', turn_str)
        return turn_str
    
    def get_turn_role(self, turn: Dict[str, Any]) -> str:
        """Get the role (user/assistant) of a conversation turn."""
        if 'role' in turn:
            return turn['role']
        elif 'sender' in turn:
            return 'user' if turn['sender'] != 'assistant' else 'assistant'
        elif 'author' in turn:
            return 'user' if turn['author'] != 'assistant' else 'assistant'
        else:
            # Try to infer from content patterns
            content = self.extract_content(turn)
            if any(marker in content.lower() for marker in ['please', 'can you', 'help me', 'i need']):
                return 'user'
            else:
                return 'assistant'
    
    def calculate_importance_weight(self, turn: Dict[str, Any]) -> float:
        """Calculate importance weight for a turn based on content analysis."""
        content = self.extract_content(turn).lower()
        role = self.get_turn_role(turn)
        
        max_weight = 0.0
        
        # Check for decision markers
        if any(re.search(marker, content) for marker in DECISION_MARKERS):
            max_weight = max(max_weight, IMPORTANCE_WEIGHTS['decision_point'])
        
        # Check for preference markers
        if any(re.search(marker, content) for marker in PREFERENCE_MARKERS):
            max_weight = max(max_weight, IMPORTANCE_WEIGHTS['user_preference'])
        
        # Check for insight markers
        if any(re.search(marker, content) for marker in INSIGHT_MARKERS):
            max_weight = max(max_weight, IMPORTANCE_WEIGHTS['novel_insight'])
        
        # User corrections/feedback
        if role == 'user' and any(word in content for word in ['wrong', 'incorrect', 'actually', 'fix', 'correct']):
            max_weight = max(max_weight, IMPORTANCE_WEIGHTS['error_correction'])
        
        # Cross-references (mentions of files, tools, other topics)
        if any(pattern in content for pattern in ['/', '.md', '.py', '.json', 'tool', 'script', 'command']):
            max_weight = max(max_weight, IMPORTANCE_WEIGHTS['cross_reference'])
        
        # Acknowledgments and pleasantries
        if content.strip() in ['ok', 'thanks', 'good', 'yes', 'no', 'sure']:
            max_weight = max(max_weight, IMPORTANCE_WEIGHTS['acknowledgment'])
        
        # Default to procedural if no other category matched
        if max_weight == 0.0:
            max_weight = IMPORTANCE_WEIGHTS['procedural']
        
        return max_weight
    
    def extract_decisions(self) -> None:
        """Extract decisions made during the conversation."""
        for i, turn in enumerate(self.turns):
            content = self.extract_content(turn)
            role = self.get_turn_role(turn)
            
            # Look for decision markers
            for marker in DECISION_MARKERS:
                if re.search(marker, content, re.IGNORECASE):
                    # Extract context around the decision
                    decision_context = self._extract_decision_context(i)
                    
                    decision = {
                        'turn': i + 1,
                        'role': role,
                        'type': 'decision',
                        'content': content[:500],  # Limit content length
                        'description': self._summarize_decision(content),
                        'context': decision_context,
                        'timestamp': turn.get('timestamp', ''),
                        'importance': self.calculate_importance_weight(turn)
                    }
                    self.decisions.append(decision)
                    break  # One decision per turn
    
    def extract_preferences(self) -> None:
        """Extract user preferences expressed during conversation."""
        for i, turn in enumerate(self.turns):
            content = self.extract_content(turn)
            role = self.get_turn_role(turn)
            
            # Focus on user turns for preferences
            if role != 'user':
                continue
            
            for marker in PREFERENCE_MARKERS:
                if re.search(marker, content, re.IGNORECASE):
                    preference = {
                        'turn': i + 1,
                        'preference': self._extract_preference_statement(content),
                        'domain': self._classify_preference_domain(content),
                        'context': content[:300],
                        'timestamp': turn.get('timestamp', ''),
                        'confidence': 0.8  # Default confidence
                    }
                    
                    # Adjust confidence based on clarity
                    if any(strong in content.lower() for strong in ['always', 'never', 'hate', 'love']):
                        preference['confidence'] = 0.9
                    elif any(weak in content.lower() for weak in ['maybe', 'perhaps', 'might']):
                        preference['confidence'] = 0.6
                    
                    self.preferences.append(preference)
                    break  # One preference per turn
    
    def extract_insights(self) -> None:
        """Extract emergent insights that span multiple turns."""
        # Pattern detection across multiple turns
        theme_patterns = {}
        
        for i, turn in enumerate(self.turns):
            content = self.extract_content(turn).lower()
            
            # Track recurring themes
            for marker in INSIGHT_MARKERS:
                if re.search(marker, content):
                    insight_text = self._extract_insight_statement(self.extract_content(turn))
                    
                    insight = {
                        'turn': i + 1,
                        'insight': insight_text,
                        'type': 'emergent_pattern',
                        'turns_involved': [i + 1],  # Start with current turn
                        'confidence': 0.7
                    }
                    
                    # Look for supporting evidence in nearby turns
                    supporting_turns = self._find_supporting_turns(i, insight_text)
                    insight['turns_involved'].extend(supporting_turns)
                    
                    # Increase confidence if pattern spans multiple turns
                    if len(insight['turns_involved']) > 1:
                        insight['confidence'] = min(0.9, 0.7 + 0.1 * len(insight['turns_involved']))
                    
                    self.insights.append(insight)
    
    def extract_reasoning_chains(self) -> None:
        """Extract reasoning chains for complex decisions."""
        for i, turn in enumerate(self.turns):
            content = self.extract_content(turn)
            
            # Look for reasoning indicators
            if any(word in content.lower() for word in ['because', 'since', 'therefore', 'so', 'given that']):
                chain = self._extract_reasoning_chain(i)
                if chain and len(chain['chain']) > 1:
                    self.reasoning_chains.append(chain)
    
    def _extract_decision_context(self, turn_idx: int, context_window: int = 2) -> List[str]:
        """Extract context around a decision turn."""
        start = max(0, turn_idx - context_window)
        end = min(len(self.turns), turn_idx + context_window + 1)
        
        context = []
        for i in range(start, end):
            if i != turn_idx:  # Don't include the decision turn itself
                content = self.extract_content(self.turns[i])
                if content and len(content) > 20:  # Filter out very short responses
                    context.append(content[:200])  # Limit context length
        
        return context
    
    def _summarize_decision(self, content: str) -> str:
        """Create a concise summary of a decision."""
        # Simple extraction of the key decision point
        sentences = content.split('.')
        for sentence in sentences:
            if any(re.search(marker, sentence, re.IGNORECASE) for marker in DECISION_MARKERS):
                return sentence.strip()[:150]
        
        # Fallback: return first sentence
        return sentences[0].strip()[:150] if sentences else content[:150]
    
    def _extract_preference_statement(self, content: str) -> str:
        """Extract the key preference statement."""
        sentences = content.split('.')
        for sentence in sentences:
            if any(re.search(marker, sentence, re.IGNORECASE) for marker in PREFERENCE_MARKERS):
                return sentence.strip()[:200]
        
        return content[:200]  # Fallback
    
    def _classify_preference_domain(self, content: str) -> str:
        """Classify the domain of a preference."""
        content_lower = content.lower()
        
        # Domain keywords
        domains = {
            'communication': ['format', 'explain', 'answer', 'response', 'tell', 'say'],
            'tools': ['tool', 'command', 'script', 'software', 'app'],
            'interface': ['ui', 'interface', 'gui', 'cli', 'terminal', 'web'],
            'workflow': ['process', 'workflow', 'method', 'approach', 'way'],
            'technical': ['code', 'programming', 'technical', 'architecture', 'design'],
            'personal': ['work', 'schedule', 'time', 'family', 'home']
        }
        
        for domain, keywords in domains.items():
            if any(keyword in content_lower for keyword in keywords):
                return domain
        
        return 'general'
    
    def _extract_insight_statement(self, content: str) -> str:
        """Extract the key insight from content."""
        # Look for sentences containing insight markers
        sentences = content.split('.')
        for sentence in sentences:
            if any(re.search(marker, sentence, re.IGNORECASE) for marker in INSIGHT_MARKERS):
                return sentence.strip()[:250]
        
        return content[:250]
    
    def _find_supporting_turns(self, base_turn: int, insight_text: str, window: int = 5) -> List[int]:
        """Find turns that support an insight pattern."""
        supporting = []
        
        # Keywords from the insight
        insight_keywords = set(re.findall(r'\b\w{4,}\b', insight_text.lower()))
        
        start = max(0, base_turn - window)
        end = min(len(self.turns), base_turn + window + 1)
        
        for i in range(start, end):
            if i == base_turn:
                continue
            
            content = self.extract_content(self.turns[i]).lower()
            content_keywords = set(re.findall(r'\b\w{4,}\b', content))
            
            # Check for keyword overlap
            overlap = insight_keywords.intersection(content_keywords)
            if len(overlap) >= 2:  # At least 2 shared keywords
                supporting.append(i + 1)
        
        return supporting
    
    def _extract_reasoning_chain(self, turn_idx: int) -> Optional[Dict[str, Any]]:
        """Extract a reasoning chain starting from a turn."""
        content = self.extract_content(self.turns[turn_idx])
        
        # Simple reasoning chain extraction
        chain_parts = []
        
        # Split on reasoning connectors
        parts = re.split(r'\b(?:because|since|therefore|so|given that|thus|hence)\b', content, flags=re.IGNORECASE)
        
        if len(parts) > 1:
            topic = parts[0].strip()[:100]
            reasoning_steps = [part.strip()[:150] for part in parts[1:] if part.strip()]
            
            if reasoning_steps:
                return {
                    'topic': topic,
                    'chain': reasoning_steps,
                    'turn': turn_idx + 1,
                    'strength': min(len(reasoning_steps) / 3.0, 1.0)  # Normalized strength
                }
        
        return None
    
    def analyze(self) -> Dict[str, Any]:
        """Perform complete analysis and return structured results."""
        print("Extracting decisions...")
        self.extract_decisions()
        
        print("Extracting preferences...")
        self.extract_preferences()
        
        print("Extracting insights...")
        self.extract_insights()
        
        print("Extracting reasoning chains...")
        self.extract_reasoning_chains()
        
        # Calculate summary statistics
        total_turns = len(self.turns)
        user_turns = sum(1 for turn in self.turns if self.get_turn_role(turn) == 'user')
        assistant_turns = total_turns - user_turns
        
        # Calculate importance distribution
        importance_scores = [self.calculate_importance_weight(turn) for turn in self.turns]
        avg_importance = sum(importance_scores) / len(importance_scores) if importance_scores else 0.0
        
        return {
            'metadata': {
                'analysis_timestamp': datetime.now().isoformat(),
                'total_turns': total_turns,
                'user_turns': user_turns,
                'assistant_turns': assistant_turns,
                'average_importance': round(avg_importance, 3)
            },
            'decisions': self.decisions,
            'preferences': self.preferences,
            'insights': self.insights,
            'reasoning_chains': self.reasoning_chains,
            'summary': {
                'decisions_found': len(self.decisions),
                'preferences_found': len(self.preferences), 
                'insights_found': len(self.insights),
                'reasoning_chains_found': len(self.reasoning_chains),
                'high_importance_turns': sum(1 for score in importance_scores if score > 0.7)
            }
        }


def generate_extraction_prompt() -> str:
    """Generate a prompt for Claude to extract insights from conversation."""
    return """You are an expert at analyzing conversations to extract key insights, decisions, and patterns. Your task is to analyze a conversation and extract:

**1. DECISIONS MADE**
- Explicit choices between alternatives
- Commitments to specific approaches
- Problem resolutions
- Strategic directions chosen

**2. USER PREFERENCES EXPRESSED** 
- Communication style preferences
- Tool/technology preferences
- Workflow preferences
- Personal working style indicators

**3. EMERGENT INSIGHTS**
- Patterns that span multiple conversation turns
- Behavioral tendencies observed
- Meta-insights about effective collaboration
- Recurring themes or motifs

**4. REASONING CHAINS**
- How complex decisions were reached
- Problem → Analysis → Decision sequences
- Supporting evidence and rationale

**FORMAT YOUR RESPONSE AS JSON:**
```json
{
  "decisions": [
    {
      "type": "choice|commitment|resolution",
      "description": "Brief description of what was decided",
      "context": "Why this decision was made",
      "importance": 0.1-1.0
    }
  ],
  "preferences": [
    {
      "domain": "communication|tools|workflow|technical|personal",
      "preference": "Description of the preference",
      "confidence": 0.1-1.0
    }
  ],
  "insights": [
    {
      "insight": "Description of the pattern or insight",
      "type": "behavioral|meta|recurring_theme|collaboration",
      "confidence": 0.1-1.0
    }
  ],
  "reasoning_chains": [
    {
      "topic": "What was being reasoned about",
      "chain": ["Step 1", "Step 2", "Step 3", "Conclusion"]
    }
  ]
}
```

**ANALYSIS PRINCIPLES:**
- Focus on preserving semantic meaning, not raw text
- Prioritize decisions and preferences (highest importance)
- Look for cross-turn patterns and dependencies
- Extract the 'why' behind decisions, not just the 'what'
- Be concise but preserve essential context

Now analyze this conversation and extract insights:"""


def main():
    parser = argparse.ArgumentParser(description='Extract insights from conversation')
    parser.add_argument('--input', required=True, help='Input conversation JSONL file')
    parser.add_argument('--output', required=True, help='Output insights JSON file')
    parser.add_argument('--workspace', help='Workspace path (default: ${ALETHEIA_NOUS:-${ALETHEIA_ROOT:-/mnt/ssd/aletheia}/nous}/syn)')
    parser.add_argument('--prompt', action='store_true', help='Output extraction prompt')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    
    args = parser.parse_args()
    
    if args.prompt:
        print(generate_extraction_prompt())
        return
    
    try:
        # Initialize analyzer
        analyzer = ConversationAnalyzer(workspace=args.workspace)
        
        # Load and analyze conversation
        if args.verbose:
            print(f"Loading conversation from: {args.input}")
        
        analyzer.load_conversation(args.input)
        
        if args.verbose:
            print(f"Loaded {len(analyzer.turns)} conversation turns")
        
        # Perform analysis
        results = analyzer.analyze()
        
        # Save results
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2)
        
        # Print summary
        if args.verbose:
            print(f"\nExtraction complete:")
            print(f"  Decisions: {results['summary']['decisions_found']}")
            print(f"  Preferences: {results['summary']['preferences_found']}")
            print(f"  Insights: {results['summary']['insights_found']}")
            print(f"  Reasoning chains: {results['summary']['reasoning_chains_found']}")
            print(f"  High importance turns: {results['summary']['high_importance_turns']}")
            print(f"\nResults saved to: {args.output}")
        else:
            print(f"Extracted {results['summary']['decisions_found']} decisions, {results['summary']['preferences_found']} preferences, {results['summary']['insights_found']} insights")
    
    except Exception as e:
        print(f"Error: {e}")
        exit(1)


if __name__ == '__main__':
    main()