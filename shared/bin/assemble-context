#!/usr/bin/env python3
"""
assemble-context â€” Dynamic context assembly for session start.

Instead of static file injection, assembles optimal context from:
  1. Session state (what was I doing?)
  2. Relevant facts (what do I need to know?)
  3. Calendar/time awareness (what's happening today?)
  4. Recent activity (what changed since last session?)
  5. Task state (what's pending?)

Writes a compiled context block that the nous can read at session start.

Usage:
  assemble-context --nous syn         # Full assembly
  assemble-context --nous syn --brief # Minimal resumption only
"""

import argparse
import json
import os
import subprocess
from datetime import datetime, timedelta
from pathlib import Path

ROOT = Path(os.environ.get("ALETHEIA_ROOT", "/mnt/ssd/aletheia"))
NOUS = ROOT / "nous"
SHARED = ROOT / "shared"
FACTS_FILE = SHARED / "memory" / "facts.jsonl"


def load_session_state(agent_id):
    state_file = NOUS / agent_id / "memory" / "session-state.yaml"
    if not state_file.exists():
        return {}
    state = {}
    with open(state_file) as f:
        for line in f:
            line = line.strip()
            if ': ' in line and not line.startswith('#') and not line.startswith('-'):
                key, val = line.split(': ', 1)
                state[key.strip()] = val.strip()
    return state


def load_recent_facts(agent_id, limit=20):
    """Load most recent facts relevant to this agent."""
    if not FACTS_FILE.exists():
        return []
    
    facts = []
    with open(FACTS_FILE) as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                fact = json.loads(line)
                facts.append(fact)
            except json.JSONDecodeError:
                continue
    
    # Sort by recency, return latest
    facts.sort(key=lambda f: f.get("valid_from", ""), reverse=True)
    return facts[:limit]


def get_recent_daily_notes(agent_id, days=2):
    """Get content from recent daily memory files."""
    memory_dir = NOUS / agent_id / "memory"
    notes = []
    for i in range(days):
        date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
        note_file = memory_dir / f"{date}.md"
        if note_file.exists():
            content = note_file.read_text().strip()
            # Truncate long files
            if len(content) > 2000:
                content = content[:2000] + "\n...(truncated)"
            notes.append({"date": date, "content": content})
    return notes


def get_pending_tasks(agent_id):
    """Get pending tasks from taskwarrior."""
    try:
        result = subprocess.run(
            ["task", "project:aletheia", "status:pending", "export"],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            tasks = json.loads(result.stdout)
            return [{"desc": t["description"], "priority": t.get("priority", ""), 
                     "due": t.get("due", "")} for t in tasks[:10]]
    except Exception:
        pass
    return []


def get_domain_knowledge(agent_id, limit=8):
    """Get high-confidence domain-specific knowledge from Neo4j graph."""
    agent_domains = {
        "syn": "cognition", "demiurge": "craft",
        "eiron": "school", "syl": "home", "arbor": "work", "akron": "vehicle",
    }
    domain = agent_domains.get(agent_id, "general")
    try:
        cypher = (
            "MATCH (s:`__Entity__`)-[r]->(o:`__Entity__`) "
            "WHERE s.domain = $domain AND r.confidence >= 0.7 "
            "RETURN s.name, type(r), o.name "
            "ORDER BY r.confidence DESC LIMIT $limit"
        )
        result = subprocess.run(
            ["aletheia-graph", "query", cypher,
             "--param", f"domain={domain}", "--param", f"limit={limit}"],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0 and result.stdout.strip():
            knowledge = []
            for line in result.stdout.strip().split("\n"):
                line = line.strip()
                if line and line != "(no results)" and "|" in line:
                    parts = [p.strip() for p in line.split("|")]
                    if len(parts) >= 3:
                        knowledge.append(f"{parts[0]} \u2014{parts[1]}\u2192 {parts[2]}")
            return knowledge
    except Exception:
        pass
    return []

def get_graph_context(agent_id, limit=10):
    """Get recent cross-domain insights from the shared graph.
    Also reinforces accessed facts (confidence bump)."""
    try:
        result = subprocess.run(
            ["aletheia-graph", "recent", "--hours", "48"],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0 and result.stdout.strip():
            lines = result.stdout.strip().split('\n')
            skip = {'s.name', 'type(r)', 'o.name', 'r.agent', 'r.timestamp', 
                    'Cached execution:', 'Query internal'}
            data = [l.strip() for l in lines 
                    if l.strip() and not any(l.strip().startswith(s) for s in skip)]
            triples = []
            reinforce_names = []
            i = 0
            while i + 4 < len(data):
                triples.append(f"{data[i]} â€”{data[i+1]}â†’ {data[i+2]}")
                reinforce_names.append((data[i], data[i+1], data[i+2]))
                i += 5

            # Confidence reinforcement handled by Neo4j natively
            return triples[:limit]
    except Exception:
        pass
    return []


def get_graph_discoveries(agent_id, limit=5):
    """Get hypothetical connections and serendipity finds relevant to this agent."""
    discoveries = []
    
    # Map agent domains
    agent_domains = {
        "syn": ["cognition", "infrastructure", "general"],
        "demiurge": ["craft"],
        "eiron": ["school"],
        "syl": ["home"],
        "arbor": ["work"],
        "akron": ["vehicle"],
    }
    domains = agent_domains.get(agent_id, ["general"])
    
    # Check serendipity finds
    ser_path = SHARED / "memory" / "graph-serendipity.jsonl"
    if ser_path.exists():
        with open(ser_path) as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    find = json.loads(line)
                    # Include if either domain matches this agent
                    if (find.get("source_domain") in domains or 
                        find.get("target_domain") in domains):
                        score = find.get("score", 0)
                        if score > 0.2:
                            discoveries.append(
                                f"ðŸ”® {find['source']} ({find['source_domain']}) "
                                f"â†” {find['target']} ({find['target_domain']}) "
                                f"via {find['bridge']} [score={score:.2f}]"
                            )
                except (json.JSONDecodeError, KeyError):
                    continue
    
    # Check link predictions
    pred_path = SHARED / "memory" / "graph-predictions.jsonl"
    if pred_path.exists():
        with open(pred_path) as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    pred = json.loads(line)
                    if pred.get("confidence", 0) > 0.3:
                        shared = pred.get("shared_via", [])
                        discoveries.append(
                            f"âš¡ Predicted link: {pred['source']} â†” {pred['target']} "
                            f"(via {', '.join(shared[:2])})"
                        )
                except (json.JSONDecodeError, KeyError):
                    continue
    
    return discoveries[:limit]


def get_calendar_context():
    """Get today's calendar events from all calendars (parallel)."""
    import concurrent.futures
    calendars = [
        ("personal", "user@example.com"),
        ("family", "family@group.calendar.example.com"),
        ("work", "cforkwright@employer.example.com"),
    ]

    def fetch_cal(name_id):
        name, cal_id = name_id
        try:
            result = subprocess.run(
                ["gcal", "today", "-c", cal_id],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0 and result.stdout.strip():
                return f"[{name}]\n{result.stdout.strip()}"
        except Exception:
            pass
        return None

    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = {executor.submit(fetch_cal, c): c for c in calendars}
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            if result:
                results.append(result)

    return "\n".join(results) if results else None


def assemble(agent_id, brief=False):
    """Assemble context for an agent."""
    import concurrent.futures
    parts = []
    now = datetime.now()
    
    # Header
    parts.append(f"# Context Assembly â€” {agent_id}")
    parts.append(f"Generated: {now.strftime('%Y-%m-%d %H:%M')} CST")
    parts.append("")
    
    # Session state (most important for continuity)
    state = load_session_state(agent_id)
    if state:
        parts.append("## Session State")
        if state.get("focus"):
            parts.append(f"Last focus: {state['focus']}")
        if state.get("last_updated"):
            parts.append(f"Last active: {state['last_updated']}")
        if state.get("open_threads"):
            parts.append(f"Open threads: {state['open_threads']}")
        if state.get("pending_decisions"):
            parts.append(f"Pending: {state['pending_decisions']}")
        parts.append("")
    
    if brief:
        return "\n".join(parts)
    
    # Parallel I/O: fetch calendar, graph, tasks, and domain knowledge simultaneously
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        cal_future = executor.submit(get_calendar_context)
        graph_future = executor.submit(get_graph_context, agent_id)
        tasks_future = executor.submit(get_pending_tasks, agent_id)
        discoveries_future = executor.submit(get_graph_discoveries, agent_id)
        domain_future = executor.submit(get_domain_knowledge, agent_id)
        
        cal = cal_future.result()
        graph_data = graph_future.result()
        tasks = tasks_future.result()
        discoveries = discoveries_future.result()
        domain_knowledge = domain_future.result()
    
    # Recent facts (decisions, preferences, corrections)
    facts = load_recent_facts(agent_id, limit=15)
    if facts:
        recent_decisions = [f for f in facts if f.get("category") == "decision"]
        recent_prefs = [f for f in facts if f.get("category") == "preference"]
        recent_insights = [f for f in facts if f.get("category") == "insight"]
        
        if recent_decisions:
            parts.append("## Recent Decisions")
            for f in recent_decisions[:5]:
                parts.append(f"- {f['object']}")
            parts.append("")
        
        if recent_prefs:
            parts.append("## Active Preferences")
            for f in recent_prefs[:5]:
                parts.append(f"- {f['subject']}: {f['object']}")
            parts.append("")
        
        if recent_insights:
            parts.append("## Recent Insights")
            for f in recent_insights[:5]:
                parts.append(f"- {f['object']}")
            parts.append("")
    
    # Tasks (already fetched in parallel above)
    if tasks:
        parts.append("## Pending Tasks")
        for t in tasks[:7]:
            priority = f" [{t['priority']}]" if t.get('priority') else ""
            parts.append(f"- {t['desc']}{priority}")
        parts.append("")
    
    # Calendar (already fetched in parallel above)
    if cal:
        parts.append("## Today's Calendar")
        parts.append(cal)
        parts.append("")
    
    # Graph insights (already fetched in parallel above)
    if graph_data:
        parts.append("## Shared Knowledge (recent)")
        for triple in graph_data:
            parts.append(f"- {triple}")
        parts.append("")
    
    # Domain knowledge (already fetched in parallel above)
    if domain_knowledge:
        parts.append("## Domain Knowledge")
        for k in domain_knowledge:
            parts.append(f"- {k}")
        parts.append("")
    
    # Graph discoveries (already fetched in parallel above)
    if discoveries:
        parts.append("## Graph Discoveries")
        for d in discoveries:
            parts.append(f"- {d}")
        parts.append("")
    
    # Recent daily notes (abbreviated)
    notes = get_recent_daily_notes(agent_id, days=1)
    if notes:
        parts.append("## Recent Activity")
        for note in notes:
            # Just the headers from daily notes
            lines = note["content"].split("\n")
            headers = [l for l in lines if l.startswith("#")]
            if headers:
                for h in headers[:10]:
                    parts.append(f"  {h}")
        parts.append("")
    
    return "\n".join(parts)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--nous", "--agent", "-a", default="syn")
    parser.add_argument("--brief", "-b", action="store_true")
    parser.add_argument("--output", "-o", help="Write to file instead of stdout")
    args = parser.parse_args()
    
    context = assemble(args.nous, brief=args.brief)
    
    if args.output:
        Path(args.output).write_text(context)
        print(f"Written to {args.output}")
    else:
        print(context)


if __name__ == "__main__":
    main()
